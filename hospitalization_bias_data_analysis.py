# -*- coding: utf-8 -*-
"""Hospitalization Bias Data Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-OgM8Jqa3e9Daqxn4W9KmkbxJNOvqE28

================================================================
##**Predict early readmission for diabetes patients based on their laboratory and medications record and mitigate the effects of bias**

### 1. Load and clean the dataset###


The dataset we use is provided by the Health Facts, while it is extracted and analyzed in the following research work: Beata Strack, Jonathan P. DeShazo, Chris Gennings, Juan L. Olmo, Sebastian Ventura, Krzysztof J. Cios, John N. Clore, "Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records", BioMed Research International, vol. 2014, Article ID 781670, 11 pages, 2014. https://doi.org/10.1155/2014/781670

The list of attributes is explained here: https://www.hindawi.com/journals/bmri/2014/781670/tab1/

We will perform data verification based on the paper’s description and conduct data preparation by mostly following the steps used in their work while performing modifications as indicated in each section for the sake of this exercise.

**Action Item 1: Data Analysis**

### 2. Train and evaluate a prediction model###

We will use scikit-learn to conduct feature engineering and model training

We train a classifier to predict early readmission based on encounter records except specific sensitive features. There are both numerical and categorical features in the dataset and we will apply appropriate feature transformation techniques before fitting into the model. Here we will also focus on how to use sklearn.pipeline.Pipeline to assemble the operations.


**Action Item 2: Analysis of Metrics**

Once we obtain the model, we will explore a series of performance metrics and observes how they serve differently when it comes to model evaluation for our problem, especially given its extremely unbalanced rate between positive and negative outcomes.

### 3 and 4. Fairness analysis and bias mitigation###

We will focus on fairness analysis unsing Fairlearn, which is a Python toolkit to assist in assessing and mitigating unfairness in machine learning models. 

In particular, we will: 

* Assess  the model's sensitivity in terms of various fairness and accuracy metrics

  Sensitive features relevant to our task include patients’ race, gender and age. We will use modules provided in Fairlearn to assess model's sensitivity when it comes to any individual of these features, or under the intersection of them. We will also quantify disparities among subgroups with different methods provided. Furthermore, we will introduce the concept about control features and how you can conduct relevant analysis with Fairlearn.

  **Action Item 3: Analysis of Group Fairness**


* Mitigate the unfairness observed in the model

  There are three groups of mitigation methods provided in Fairlearn: preprocessing, reduction and postprocessing. We select two specific algorithms to mitigate the observed unfairness in the model obtained from the first part.

  **Action Item 4: Analysis of Fairness Mitigation**

---------------

This assignment is based on a Lab given in the Responsible Data Science class at NYU by Julia Stoyanovich, and on a Fairlearn tutorial by  Roman Lutz, who is one of the main contributors to Fairlearn.
"""

# Commented out IPython magic to ensure Python compatibility.
### Install and import packages

!pip install --upgrade fairlearn==v0.7.0

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# %matplotlib inline

"""--------
--------

1.Load and clean the data
====
"""

from google.colab import drive
drive.mount('/content/drive')
data = pd.read_csv('/content/drive/MyDrive/diabetic_data_initial.csv')

from google.colab import drive
drive.mount('/content/drive')

data.head()

"""_____
Let's do some Data Cleaning based on the Strack et al. analysis
-------

The admission_type_id does not tell us much, we can map it to the actual meaning.

There were several features that could not be treated directly since they had a high percentage of missing values. These features were weight (97% values missing), payer code (40%), and medical specialty (47%).

The preliminary dataset contained multiple inpatient visits for some patients 
  and the observations could not be considered as statistically independent, 
  an assumption of the logistic regression model.

  Authors of the paper performed a specific filtering criteria but here for simplicity, 
  we drop all records from patients who have more than one encounter.

  After performing the above operation, 
   we were left with 54,745 encounters that constituted the final dataset for analysis.
"""

# Map admission_type_ide id to the actual meaning of admission type and create a column named 'admission_type' in the data frame.
names = ['Emergency', 'Urgent', 'Elective', 'Newborn', 'Not Available', 'NULL', 'Trauma Center', 'Not Mapped']
data['admission_type'] = [names[i-1] for i in data.admission_type_id]
data.head()

# let's drop records with missing values for gender 
data = data.loc[data['gender']!='Unknown/Invalid']

## Features to drop based on the Strack et al. paper

# 'Weight attribute was considered to be too sparse and it was not included in further analysis.'
data = data.drop(columns='weight')
# 'Payer code was removed since it had a high percentage of missing values and it was not considered relevant to the outcome.'
data = data.drop(columns='payer_code')
# 'Medical specialty attribute was maintained, adding the value “missing” in order to account for missing values.'
# For simplicity, we drop this column instead. 
data = data.drop(columns='medical_specialty')

#check the number of encounters per patient
plt.hist(data.groupby(['patient_nbr'])['encounter_id'].nunique())
# dropping records 
encounter_counts = data.groupby(['patient_nbr'])['encounter_id'].nunique()
data = data[data.patient_nbr.isin(encounter_counts[encounter_counts.values == 1].index)]

# check the distribution again.
plt.hist(data.groupby(['patient_nbr'])['encounter_id'].nunique())

"""_____

<br>


<font color='red'>**Action Item 1**: Analyse the data</font>
-------

Look at the data. Can you identify patterns. 
Remember, we will focus on predicting readmission.
In your submission you should describe what type of analyis you did and what patterns/correlations you identified.

Below is some sample code to get you started. You can modify/add to it.
<br>
<br>

------
"""

# Get a summary of the dataset with pandas.  
data.describe()

# Analyze length of stay
plt.hist(data['time_in_hospital']) 
plt.xlabel('length of stay (days)')
plt.ylabel('counts')

# Frequency of time in hospital by various groupings
#separate plots
data['time_in_hospital'].hist(by=data['gender'])
data['time_in_hospital'].hist(by=data['age'])
data['time_in_hospital'].hist(by=data['race'])

data.groupby('gender').time_in_hospital.value_counts().unstack(0).plot.bar()
data.groupby('age').time_in_hospital.value_counts().unstack(0).plot.bar()
data.groupby('race').time_in_hospital.value_counts().unstack(0).plot.bar()
# Note that this is the count, not the frequency. The histograms above show you the frequencies

# Frequency of time in hospital by various groupings - same plot
data.groupby('gender').time_in_hospital.value_counts(normalize=True).unstack(0).plot.bar()
data.groupby('age').time_in_hospital.value_counts(normalize=True).unstack(0).plot.bar()
data.groupby('race').time_in_hospital.value_counts(normalize=True).unstack(0).plot.bar()

# Frequency of readmission  by various groupings - same plot
data.groupby('gender').readmitted.value_counts(normalize=True).unstack(0).plot.bar()
data.groupby('age').readmitted.value_counts(normalize=True).unstack(0).plot.bar()
data.groupby('race').readmitted.value_counts(normalize=True).unstack(0).plot.bar()

### Here you can add additional code to analyze the data
data.groupby('age').time_in_hospital.value_counts(normalize = True).unstack(0).plot.bar()
data.groupby('readmitted').age.value_counts(normalize = True).unstack(0).plot.bar()

data.groupby('readmitted').gender.value_counts(normalize = True).unstack(0).plot.bar()
data.groupby('gender').age.value_counts(normalize = True).unstack(0).plot.bar()
data.groupby('race').age.value_counts(normalize = True).unstack(0).plot.bar()
data.groupby('gender').time_in_hospital.value_counts(normalize = True).unstack(0).plot.bar()

data.groupby('readmitted').time_in_hospital.value_counts(normalize = True).unstack(0).plot.bar()
data.groupby('time_in_hospital').readmitted.value_counts(normalize = True).unstack(0).plot.bar()

"""--------
--------

2.Train and Evaluate a Prediction Model
====

We will use the **sklearn.pipeline.Pipeline** to perform feature engineering and to train a classifier. For the purpose of this assignment, you do not need to know how to train the classifier; we are providing you with the code.

###**We are creating a  model to predict if a patient will be readmitted**###

**scikit-learn** or **sklearn** is a python module consisting of simple and efficient tools for predictive data analysis (preprocessing, feature selection, dimensionality reduction, regression, classification, clustering, et cetera).

**sklearn.pipeline.Pipeline** object takes a list of **transforms** and a final **estimator** and applies them sequentially on data.
- The possible transforms are preprocessing (e.g., MinMaxScaler, OneHotEncoder), missing value imputation (e.g., KNNImputer, SimpleImputer), dimensionality reduction (e.g., PCA), label encoding (e.g., LabelBinarizer), etc. Any transformers in the pipeline must implement fit and transform methods. For a more complete list, see: https://scikit-learn.org/stable/data_transforms.html

- The final part of the pipeline, the estimator, needs to implement a fit function at the least. The estimators can be classifiers, regressors, etc. For a more complete list, see: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html

The purpose of setting up a pipeline is to allow the different transforms and estimator to be cross-validated together when setting their parameters. It also allows for the application of a pre-configured pipeline on new raw data. The different steps of the pipeline and their parameters can be accessed using their 'names'.
"""

#Prepare the target variable Y: is a patient readmitted
'''
Target variable, readmitted, is binarized as below: 
  =1:  “readmitted” if the patient was readmitted within 30 days of discharge or 
  =0:   “otherwise” which covers both readmission after 30 days and no readmission at all. 
''' 
Y = (data['readmitted']=='<30').astype('int')
print(f'Readmitted rate within 30 days of discharge in the dataset={sum(Y==1)/len(Y)*100:1.3f}%')

#Define Sensitive and control variables including demographics, severity and type of the disease, and type of admission.
sensitive_control_features = ['race', 'gender', 'age', 
          'admission_type', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id',
          ]
A = data[sensitive_control_features]
# Remove from the data the target and identifying features (encounter_id, and patient_nbr), as well as the sensitive and control features
X_raw = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'] + sensitive_control_features)

# Define the model
from sklearn.model_selection import train_test_split

#Denoting categorical variables in the data 
for col in ['max_glu_serum', 'A1Cresult',
       'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',
       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',
       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',
       'tolazamide', 'examide', 'citoglipton', 'insulin',
       'glyburide-metformin', 'glipizide-metformin',
       'glimepiride-pioglitazone', 'metformin-rosiglitazone',
       'metformin-pioglitazone', 'change', 'diabetesMed',
       'time_in_hospital', 'num_lab_procedures', 'num_procedures',
          'num_medications', 'number_outpatient', 'number_emergency',
          'number_inpatient', 'number_diagnoses',
          'diag_1', 'diag_2', 'diag_3',
       ]:

    X_raw[col] = X_raw[col].astype('category')

## Split the data between training (X_train) and test(X_test) data
(X_train, X_test, y_train, y_test, A_train, A_test) = train_test_split(
    X_raw, Y, A, test_size=0.5, random_state=10000, stratify=Y
)

# Ensure indices are aligned between X, y and A,
# after all the slicing and splitting of DataFrames
# and Series

X_train = X_train.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)
y_test = y_test.reset_index(drop=True)
A_train = A_train.reset_index(drop=True)
A_test = A_test.reset_index(drop=True)

## Train a classifier to predict if a patient will be admitted based on the non-sensitive non-control non-identifying features
## Name of the classifier is unmitigated_predictor

from sklearn.linear_model import LogisticRegression

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.compose import make_column_selector as selector
from sklearn.pipeline import Pipeline

unmitigated_predictor = Pipeline(
    steps=[
           #feature engineering component 
        ("preprocessor", ColumnTransformer(transformers=[
                                              # we use selector to indentify features based on its data type
                                              # Normalize numerical features
                                               ("num", StandardScaler(), selector(dtype_exclude="category")), 
                                              # Encoding (transforming) categorical features to what understandable by the model
                                              ("cat", OneHotEncoder(handle_unknown="ignore"), selector(dtype_include="category")),
                                            ]
                                          )
        ),
        # model component
        ("classifier",
         LogisticRegression(penalty='l2', C=0.1, verbose=0, max_iter=1000, 
                            class_weight={0:1, 1:10}),
        ),
    ]
)

"""## Performance and fairness metrics of the Classifier (prediction model).

Sklearn compiles a set of metrics to assess the quality of the classifier. We will use the following, which we discussed in class:


* Sample size
* Selection rate, percentage of positive cases:  fraction of predicted labels matching the ‘good’ outcome
* Accuracy: $\frac{TP + TN}{P + N}$
* FNR - False Negative Rate, percentage among readmitted that were predicted not to be readmitted (patients that could have used extra attention but did not get it): $\frac{FN}{FN + TP}$
* FPR - False Positive Rate, percentage among not readmitted that were predicted to be readmitted ("unnecessarily getting extra attention" - these people take resources that the hospital could have given to patients in need): $\frac{FP}{FP + TN}$
* Average Precision: Summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight
  * Precision: number of patients that were readmitted among those who were predicted to be readmitted: $\frac{TP}{TP + FP}$. High precision relates to a low FPR. 
  * Recall: number of patients that were readmitted among those needed to be readmitted $\frac{TP}{TP + FN}$. High recall relates to a low FNR. 
  * To learn more about precision and recall, visit 
  https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html
* ROC AUC score: Computes Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. 
  * ROC curve is defined by FPR (x values) and TPR (y values). The purpose of the graph is to illustrate the trade-offs between TP and FP. 
  * To learn more about ROC curve, visit https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py
"""

import sklearn.metrics as skm

unmitigated_predictor.fit(X_train, y_train)
y_pred = unmitigated_predictor.predict(X_test)

# get the overall performance metrics of the classifer
print("accuracy:", skm.accuracy_score(y_test, y_pred))
print("AUC:", skm.roc_auc_score(y_test, y_pred))
print('precision:', skm.precision_score(y_test, y_pred))
print('recall:', skm.recall_score(y_test, y_pred))
print('average_precision:', skm.average_precision_score(y_test, y_pred))
fpr, fnr, thresholds = skm.det_curve(y_test, y_pred)
print('FPR:', fpr[1])
print('FNR:', fnr[1])

"""_____

<br>


<font color='red'>**Action Item 2**: Analyse the performance metrics</font>
-------

Does the prediction perform well? 

<br>
<br>

-----

--------
--------

3.Calculate the Fairness Metrics
====


### Use **fairlearn.metrics.MetricFrame** class to inspect model's fairness

We know that there are sensitive features in our data, and we want to ensure that we’re not harming individuals due to membership in any of these groups. For this purpose, Fairlearn provides the fairlearn.metrics.MetricFrame class. 

The **fairlearn.metrics.MetricFrame** object requires a minimum of four arguments:

* The underlying metric function(s) to be evaluated
* The true values
* The predicted values
* The sensitive feature values

Metric functions must have a signature ''fn(y_true, y_pred)'', i.e., require only two arguments.
"""

from fairlearn.metrics import MetricFrame
from fairlearn.metrics import selection_rate, false_negative_rate, false_positive_rate

import functools

def insensitive_roc_auc(y_true, y_score):
    #to handle subgroups with only one class.
    if sum(y_true)!=len(y_true) and sum(y_true)!=0:
        return skm.roc_auc_score(y_true, y_score)
    else:
        return np.nan

def samplesize(y_true, y_score):
    return len(y_true)

#Metrics 
metric_fns = {'samplesize': samplesize, 
              'selection_rate': selection_rate,  # i.e., the percentage of the population which have ‘1’ as their label
              'FNR': false_negative_rate, 
              'FPR': false_positive_rate, 
              'accuracy': skm.accuracy_score,
              'average_precision': skm.average_precision_score,
              'roc_auc_score': insensitive_roc_auc 
              }

#Gender grouping
grouped_on_gender = MetricFrame(metrics=metric_fns,
                             y_true=y_test, y_pred=y_pred,
                             sensitive_features=A_test['gender'])

#We can inspect the overall values, and check they are as expected (same as computed above):
assert grouped_on_gender.overall['selection_rate'] == selection_rate(y_test, y_pred)
assert grouped_on_gender.overall['accuracy'] == skm.accuracy_score(y_test, y_pred)
print(grouped_on_gender.overall)

"""### Check Fairness by Subgroup###"""

# Gender
#grouped_on_gender = MetricFrame(metrics=metric_fns,
                           #  y_true=y_test, y_pred=y_pred,
                            # sensitive_features=A_test['gender'])   DEFINED ABOVE
grouped_on_gender.by_group

# Race
grouped_on_race = MetricFrame(metrics=metric_fns,
                             y_true=y_test, y_pred=y_pred,
                             sensitive_features=A_test['race'])
grouped_on_race.by_group

# Age
grouped_on_age = MetricFrame(metrics=metric_fns,
                             y_true=y_test, y_pred=y_pred,
                             sensitive_features=A_test['age'])
grouped_on_age.by_group

"""#### Quantifying Disparities 
**Fairlearn** provides several means of aggregating metrics across the subgroups, so that disparities can be readily quantified.
"""

'''
The simplest of these aggregations is group_min(), which reports the minimum value 
seen for a subgroup for each underlying metric (as well as group_max()). 
This is useful if there is a mandate that “no subgroup should have an accuracy() of less than 0.8.” 
'''
grouped_on_race.group_min()

'''
The difference between metrics achieved on subgroups can be quantified 
in terms of a difference between the subgroup with the highest value of the metric, and the subgroup with the lowest value.
'''
grouped_on_race.difference(method='between_groups')

'''
We can also evaluate the difference relative to the corresponding overall value of the metric. 
In this case we take the absolute value, so that the result is always positive:'
'''
grouped_on_race.difference(method='to_overall')

"""
```
# This is formatted as code
```

#### Intersections of sensitive features
We can examine the intersections of sensitive features by passing multiple columns to the fairlearn.metrics.MetricFrame constructor:"""

grouped_on_race_and_sex = MetricFrame(metrics=metric_fns,
                             y_true=y_test, y_pred=y_pred,
                                      sensitive_features=A_test[['race', 'gender']])
grouped_on_race_and_sex.by_group

"""#### Control Features
There is a further way we can slice up our data. We have features for the patients' *admission_type_id* (Integer identifier corresponding to 9 distinct values, for example, emergency, urgent, elective, newborn, and not available). When making predictions for readmission, it is acceptable that patients who were admitted due to a specific reason this time to have higher rate of readmission. However, within each admission type, we do not want a disparity between gender. To handle such senario, we introduce the concept of **control features**.

Control features are introduced by the `control_features= argument` to the `fairlearn.metrics.MetricFrame` object:
"""

cond_admissiontype = MetricFrame( metrics={'sample size': samplesize,
                          'selection_rate': selection_rate,  
                          'accuracy': skm.accuracy_score,
                          'FNR': false_negative_rate, 
                          'FPR': false_positive_rate,
                          'roc_auc_score': insensitive_roc_auc },
                            y_true=y_test, y_pred=y_pred,
                                sensitive_features=A_test[['gender']],
                                control_features=A_test['admission_type'])
#Note that we removed some metrics because some subgroups had N=0, which led to division by 0 error

'''
This has an immediate effect on the overall property. 
Instead of having one value for each metric, we now have a value for each unique value of the control feature:
'''
cond_admissiontype.overall

cond_admissiontype.by_group

"""---------"""

### Here you can add additional analysis code to analyze Fairness

"""-----
<br>


<font color='red'>**Action Item 3**: Analyze the Group Fairness</font>
-------

Is the prediction fair for different groups? 
Are there any differences in the performance that stand out to you?
Any case of Simpson's paradox?

<br>
<br>

--------

--------
--------

4.Mitigate the Unfairness of the Classifier.
====

**Fairlearn** provide preprocessing, postprocessing, and reductions modules for unfairness mitigation. For this assignment we will look at the behavior of a postprocessing method.

##Mitigating Difference with Postprocessing
We attempt to mitigate the disparities in the classifier using the Fairlearn postprocessing algorithm ThresholdOptimizer. This algorithm finds a suitable threshold for the scores (class probabilities) produced by the model by optimizing the accuracy rate under the constraint that the equalized odds (with respect to the sensitive feature and the outcome) is zero, using the training data. Here we consider race as the sensitive feature.
"""

from fairlearn.postprocessing import ThresholdOptimizer, plot_threshold_optimizer
postprocess_est = ThresholdOptimizer(
    estimator=unmitigated_predictor.named_steps['classifier'],
    constraints="false_negative_rate_parity",
    objective='balanced_accuracy_score',predict_method='predict'
    )

X_train_ = unmitigated_predictor.named_steps['preprocessor'].fit_transform(X_train)
X_test_ = unmitigated_predictor.named_steps['preprocessor'].transform(X_test)

postprocess_est.fit(X_train_.toarray(), y_train, 
                    sensitive_features=A_train['race'])
postprocess_preds = postprocess_est.predict(X_test_.toarray(),
                                            sensitive_features=A_test['race'])

plot_threshold_optimizer(postprocess_est, ax=None, show_plot=True)

"""Let's further compare the mitigated and unmitigated models' performance."""

metric_fns = {'FNR': false_negative_rate, 
              'FPR': false_positive_rate, 
              'accuracy': skm.accuracy_score,
              'average_precision': skm.average_precision_score,
              'roc_auc_score': insensitive_roc_auc 
              }

modelnames, accuracies, disparities = [], [], []


#get performance for the unmitigated_predictor 
accuracy_metric_frame = MetricFrame(metrics=metric_fns,
                             y_true=y_test, y_pred=y_pred, 
                                    sensitive_features=A_test['race'])

accuracies.append(accuracy_metric_frame.overall)
disparities.append(accuracy_metric_frame.ratio(method='to_overall'))
modelnames.append('unmitigated_predictor')

#get performance for the mitigated predictor from GridSearch
accuracy_metric_frame = MetricFrame(metrics=metric_fns,
                             y_true=y_test, y_pred=postprocess_preds,
                                    sensitive_features=A_test['race'])

accuracies.append(accuracy_metric_frame.overall)
disparities.append(accuracy_metric_frame.ratio(method='to_overall'))
modelnames.append('mitigated_predictor')

#Plot the disparities between groups and overall accuracy metrics for the unmitigated and mitigated predictors below:
pd.DataFrame(disparities, index=modelnames).T.plot.bar(figsize=(8, 4), 
                                                       title='disparities between groups')
pd.DataFrame(accuracies, index=modelnames).T.plot.bar(figsize=(8, 4), 
                                                       title='overall performance')

# Mitigated values by Race
mitigated_grouped_on_race = MetricFrame(metrics=metric_fns,
                             y_true=y_test, y_pred=postprocess_preds,
                             sensitive_features=A_test['race'])
mitigated_grouped_on_race.by_group

# Unmitigated values by Race
grouped_on_race = MetricFrame(metrics=metric_fns,
                             y_true=y_test, y_pred=y_pred,
                             sensitive_features=A_test['race'])
grouped_on_race.by_group

"""-----
<br>


<font color='red'>**Action Item 4**: Analyze the Result of Fairness Mitigation</font>
-------

How do the unmitigated and mitigated predictors compare?


<br>
<br>

--------

"""

